# Maths

## SVD

[MIT class notes on SVD](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://math.mit.edu/classes/18.095/2016IAP/lec2/SVD_Notes.pdf&ved=2ahUKEwi-wKjS7beFAxWCT6QEHZAbAWwQFnoECDsQAQ&usg=AOvVaw0X6GdgXWT--aZvuvcmo3ZM)

The regular denationalization of a rectangular matrix $A$ of size $n \times m$ by $X^{-1} AX$ the resulting eigenvectors in X will not always be orthogonal, it requires to have a matrix $A$ a square matrix to have $Ax = \lambda x$. Also we have not always enough eigenvalues (apparently, but I don't really get what this means and why).

The singular value décomposition methods enable to diagonalise a rectangular matrix $A$ of size $n \times m$ in two sets of singular vectors, u's and v's. 
$u's \in \mathbb{R}^m$ and $u's \in \mathbb{R}^n$ will be in a $m \times m$ U matrix and a $n \times n$ V matrix

$A = U \Sigma V^\intercal$
$\Leftrightarrow A = \sum_i \sigma_i U_i \otimes V_i^\intercal$
the  $\Sigma$ matrix contains only values on the diagonal and contains the $\sigma$ values that are analogous to the eigenvalues 
We call U the left singular vector and V the right singular vector of $\sigma$
the diagonal entries $\sigma_i = \Sigma_{ii}$ of $\Sigma$ are uniquely determined by $M$ and known as singular values of $M$. The number non 0 singumar values is equal to the rank of the matrix $M$.

enables to obtain a sum of the vector product of rank one matrices, enabling to approximate more and more accurately the origin matrix. 
## AFC

# Modèle



# Écologie

